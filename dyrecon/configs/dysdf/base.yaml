trial_name: base
tag: ''
name: dysdf/${dataset.scene}/${trial_name}${tag}
seed: 42

dataset:
  load_time_steps: 100
  val_load_time_steps: 2
  dynamic_sequence: True
  train_split:
    - cam_train_1
    - cam_train_3
    - cam_train_6
    - cam_train_8
  val_split: 
    - cam_test
  test_split: ${dataset.val_split}

  name: dysdf_dataset
  scene: ???
  data_root: ../DATA_ROOT/Owlii/${dataset.scene}
  sampling: ${model.sampling}

  predict:
    cam_1: cam_train_3
    cam_2: cam_train_8
    n_imgs: ${n_frames}

n_frames: ${dataset.load_time_steps}

model:
  name: Renderer
  background: black
  alpha_ratio:
    strategy: interpolated
    max_steps: 50000

  isosurface:
    resolution: 512

  sampling:
    name: uniform_sampler #importance_sampler or proposal_sampler
    strategy: balanced
    train_num_rays: 1100 #6144
    n_samples: 128
    n_importance: 128
    randomized: true
    prop_samples: [128]
    ray_chunk: ${model.sampling.train_num_rays}
    prop_net:
      model: ${model.dynamic_fields}

  deviation_net:
    name: laplace_density
    beta_min: 0.0001
    beta: 0.1

  dynamic_fields:
    name: DynamicFields
    n_frames: ${n_frames}
    ambient_dim: 0
    deform_dim: 0

    deform_net: null
    hyper_net:
      name: hyper_time_network
      d_in: 0
      d_out: 0
      multires_out: 0

    sdf_net:
      name: sdf_network
      n_frames: ${n_frames}
      resfield_layers: []
      composition_rank: 10
      d_out: 129
      d_in_1: 3
      # d_in_2: 1
      d_hidden: 128
      n_layers: 8
      skip_in: [4]
      multires: 6
      multires_topo: 0
      bias: 0.5
      scale: 1.0
      geometric_init: True
      weight_norm: False
      inside_outside: False
    
    color_net:
      name: color_network
      d_hidden: 128
      n_layers: 1
      mode: 
        feature: ${sub:${model.dynamic_fields.sdf_net.d_out}, 1}

system:
  name: dysdf_system
  loss:
    rgb: 1.0
    mask: 0.1
    eikonal: 0.1
    dist: 0.0
    depth: 0.0
    sparse: 0.0
    loss_proposal: 1.0 # if the proposal net is used
  optimizer:
    name: Adam
    args:
      lr: 0.0005 #5e-4 0.0001
  scheduler:
    name: CosineAnnealingLR
    args:
      T_max: ${trainer.max_steps}
      eta_min: ${mul:${system.optimizer.args.lr}, 0.1}

checkpoint:
  save_top_k: -1
  every_n_train_steps: 20000
  save_last: True
trainer:
  max_steps: 400000
  log_every_n_steps: 100
  num_sanity_val_steps: 0
  val_check_interval: 10000
  # limit_val_batches: 2 #0.06 # Note: this causes issues in pl 1.6.5
  enable_progress_bar: true
